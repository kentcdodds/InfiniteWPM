'use strict';
(function() {
  var app = angular.module('iwpm');
  
  app.value('defaultText', "/*\n * Generic entry point for the idle threads\n */\n#include <linux/sched.h>\n#include <linux/cpu.h>\n#include <linux/tick.h>\n#include <linux/mm.h>\n#include <linux/stackprotector.h>\n\n#include <asm/tlb.h>\n\n#include <trace/events/power.h>\n\nstatic int __read_mostly cpu_idle_force_poll;\n\nvoid cpu_idle_poll_ctrl(bool enable)\n{\n  if (enable) {\n    cpu_idle_force_poll++;\n  } else {\n    cpu_idle_force_poll--;\n    WARN_ON_ONCE(cpu_idle_force_poll < 0);\n  }\n}\n\n#ifdef CONFIG_GENERIC_IDLE_POLL_SETUP\nstatic int __init cpu_idle_poll_setup(char *__unused)\n{\n  cpu_idle_force_poll = 1;\n  return 1;\n}\n__setup(\"nohlt\", cpu_idle_poll_setup);\n\nstatic int __init cpu_idle_nopoll_setup(char *__unused)\n{\n  cpu_idle_force_poll = 0;\n  return 1;\n}\n__setup(\"hlt\", cpu_idle_nopoll_setup);\n#endif\n\nstatic inline int cpu_idle_poll(void)\n{\n  rcu_idle_enter();\n  trace_cpu_idle_rcuidle(0, smp_processor_id());\n  local_irq_enable();\n  while (!need_resched())\n    cpu_relax();\n  trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());\n  rcu_idle_exit();\n  return 1;\n}\n\n/* Weak implementations for optional arch specific functions */\nvoid __weak arch_cpu_idle_prepare(void) { }\nvoid __weak arch_cpu_idle_enter(void) { }\nvoid __weak arch_cpu_idle_exit(void) { }\nvoid __weak arch_cpu_idle_dead(void) { }\nvoid __weak arch_cpu_idle(void)\n{\n  cpu_idle_force_poll = 1;\n  local_irq_enable();\n}\n\n/*\n * Generic idle loop implementation\n */\nstatic void cpu_idle_loop(void)\n{\n  while (1) {\n    tick_nohz_idle_enter();\n\n    while (!need_resched()) {\n      check_pgt_cache();\n      rmb();\n\n      if (cpu_is_offline(smp_processor_id()))\n        arch_cpu_idle_dead();\n\n      local_irq_disable();\n      arch_cpu_idle_enter();\n\n      /*\n       * In poll mode we reenable interrupts and spin.\n       *\n       * Also if we detected in the wakeup from idle\n       * path that the tick broadcast device expired\n       * for us, we don't want to go deep idle as we\n       * know that the IPI is going to arrive right\n       * away\n       */\n      if (cpu_idle_force_poll || tick_check_broadcast_expired()) {\n        cpu_idle_poll();\n      } else {\n        current_clr_polling();\n        if (!need_resched()) {\n          stop_critical_timings();\n          rcu_idle_enter();\n          arch_cpu_idle();\n          WARN_ON_ONCE(irqs_disabled());\n          rcu_idle_exit();\n          start_critical_timings();\n        } else {\n          local_irq_enable();\n        }\n        current_set_polling();\n      }\n      arch_cpu_idle_exit();\n    }\n    tick_nohz_idle_exit();\n    schedule_preempt_disabled();\n  }\n}\n\nvoid cpu_startup_entry(enum cpuhp_state state)\n{\n  /*\n   * This #ifdef needs to die, but it's too late in the cycle to\n   * make this generic (arm and sh have never invoked the canary\n   * init for the non boot cpus!). Will be fixed in 3.11\n   */\n#ifdef CONFIG_X86\n  /*\n   * If we're the non-boot CPU, nothing set the stack canary up\n   * for us. The boot CPU already has it initialized but no harm\n   * in doing it again. This is a good place for updating it, as\n   * we wont ever return from this function (so the invalid\n   * canaries already on the stack wont ever trigger).\n   */\n  boot_init_stack_canary();\n#endif\n  current_set_polling();\n  arch_cpu_idle_prepare();\n  cpu_idle_loop();\n}");

})();